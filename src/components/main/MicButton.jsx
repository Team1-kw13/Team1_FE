import { useState, useRef, useEffect } from "react";
import micIcon from "../../assets/images/mic_fill.svg";
import stopIcon from "../../assets/images/stop.svg";
import webSocketService from "../../service/websocketService";

// ì˜¤ë””ì˜¤ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
function float32ToInt16(float32Array) {
  const int16Array = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    // Float32 (-1.0 ~ 1.0)ë¥¼ Int16 (-32768 ~ 32767)ë¡œ ë³€í™˜
    int16Array[i] = Math.max(-1, Math.min(1, float32Array[i])) * 0x7FFF;
  }
  return int16Array;
}

function arrayBufferToBase64(buffer) {
  let binary = '';
  const bytes = new Uint8Array(buffer);
  const chunkSize = 0x8000; // 32KB ì²­í¬ë¡œ ì²˜ë¦¬
  
  for (let i = 0; i < bytes.length; i += chunkSize) {
    binary += String.fromCharCode.apply(null, bytes.subarray(i, i + chunkSize));
  }
  return btoa(binary);
}

async function startAudioRecognition(onAudioData) {
  const stream = await navigator.mediaDevices.getUserMedia({ 
    audio: {
      sampleRate: 24000,
      channelCount: 1,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl:true
    }
  });
  
  const audioContext = new AudioContext({ sampleRate: 24000 });
  const source = audioContext.createMediaStreamSource(stream);
  const processor = audioContext.createScriptProcessor(4096, 1, 1);
  
  source.connect(processor);
  /*processor.connect(audioContext.destination);*/
  //destinationì— ì—°ê²°í•˜ì§€ ì•Šì•„ë„ onaudioprocessëŠ” ë™ì‘í•¨
  
  processor.onaudioprocess = (e) => {
    const inputData = e.inputBuffer.getChannelData(0);
    console.log("ğŸ¤ ì˜¤ë””ì˜¤ ìº¡ì²˜ë¨, ê¸¸ì´:", inputData.length); // âœ… ì¶”ê°€
    const pcmBuffer = float32ToInt16(inputData);
    
    // ì½œë°±ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ë‹¬
    const base64 = arrayBufferToBase64 (pcmBuffer);
    onAudioData?.(base64);
  };
  
  return { stream, audioContext, processor };
}

async function stopAudioRecognition(stream,audioContext,processor) {
  try { if (processor) { processor.onaudioprocess = null; processor.disconnect(); } } catch {}
  try { if (stream) stream.getTracks().forEach(t => t.stop()); } catch {}
  try {
    if (audioContext && audioContext.state !== 'closed') {
        await audioContext.close();
    }
  } catch {}
}

export default function MicButton({ onListeningStart, onListeningStop, onTranscriptUpdate, currentStep }) {
  const [isRecording, setIsRecording] = useState(false);
  const [transcriptText, setTranscriptText] = useState('');
  const transcriptsRef = useRef({});
  const audioSystemRef = useRef(null);
  const stoppingRef=useRef(false);
  const currentOutputIndex = useRef(0);

  useEffect(() => {
    if (currentStep !== "listening" && isRecording) {
        stopRecording();
    }
    // Speech Recognition ì´ˆê¸°í™” (í…ìŠ¤íŠ¸ ë³€í™˜ìš©)
    const handleInputTranscriptDelta = (data) => {
        console.log('ìŒì„± -> í…ìŠ¤íŠ¸ ê²°ê³¼:', transcriptText);
        console.log('ìŒì„± -> í…ìŠ¤íŠ¸ : ', data);
        
        const { output_index, delta } = data;

        if (!transcriptsRef.current[output_index]) {
            transcriptsRef.current[output_index] = '';
        }

        if (delta) {
            transcriptsRef.current[output_index] += delta;
            const newText = transcriptsRef.current[output_index];

            console.log("ëˆ„ì  í…ìŠ¤íŠ¸: ", newText);
            setTranscriptText(newText);

            if (onTranscriptUpdate) {
                onTranscriptUpdate(newText);
            }
        }
    };

    const handleInputTranscriptDone = (data) => {
        console.log('ìŒì„± ì¸ì‹ ì™„ë£Œ', data);
        const { output_index } = data;

        const finalText = transcriptsRef.current[output_index] || '';
        console.log("ìµœì¢… ì¸ì‹ ê²°ê³¼ í…ìŠ¤íŠ¸: ", finalText);

        if (finalText && onListeningStop) {
            onListeningStop(finalText.trim());
        }

        setTranscriptText('');
        currentOutputIndex.current++;
    }

    const handleError = (data) => {
        console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜', data);
        if (isRecording) {
            stopRecording(); // ì˜¤ë¥˜ ë°œìƒ ì‹œ ë…¹ìŒ ì¤‘ì§€
        }
    }

    webSocketService.on('openai:conversation', 'input_audio_transcript.delta', handleInputTranscriptDelta);
    webSocketService.on('openai:conversation', 'input_audio_transcript.done', handleInputTranscriptDone);
    webSocketService.on('openai:error', handleError);

    return () => {
      // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
      webSocketService.off('openai:conversation', 'input_audio_transcript.delta', handleInputTranscriptDelta);
      webSocketService.off('openai:conversation', 'input_audio_transcript.done', handleInputTranscriptDone);
      webSocketService.off('openai:error', handleError);

      if (audioSystemRef.current) {
        const { stream, audioContext, processor } = audioSystemRef.current;
        stopAudioRecognition(stream, audioContext, processor);
      }
    };
  }, [currentStep]);

  {/*ì‹œì‘*/}
  const startRecording = async () => {
    try {
      if(stoppingRef.current) return; //ë©ˆì¶”ëŠ” ì¤‘ì´ë©´ ë¬´ì‹œ
      // WebSocket ì—°ê²° í™•ì¸
      if (!webSocketService.isConnected) {
        console.log('WebSocket ì—°ê²° ì¤‘...');
        webSocketService.connect(import.meta.env.VITE_WEBSOCKET_URL);
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
      
      //ì„œë²„ê°€ CONNECTED ë³´ë‚¼ ë•Œê¹Œì§€ ëŒ€ê¸° 
      //await webSocketService.waitReady();
      setIsRecording(true);
      setTranscriptText(''); // ì´ì „ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
      transcriptsRef.current = {}; //ì „ì²´ ì´ˆê¸°í™”

      // ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì— ë…¹ìŒ ì‹œì‘ ì•Œë¦¼
      onListeningStart?.()

      // WebSocketìœ¼ë¡œ ìŒì„± ë°œí™” ì‹œì‘ ì•Œë¦¼
      webSocketService.startSpeaking();

      // ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œì‘
      const audioSystem = await startAudioRecognition((chunk) => {
        const success = webSocketService.sendAudioBuffer(chunk);

        if (!success) {
            console.error("ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡ ì‹¤íŒ¨");
        }
      });
      
      audioSystemRef.current = audioSystem;
    }
    catch (error) {
      console.error('ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:', error);
      setIsRecording(false);
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
    }
  };

  {/*ì •ì§€*/}
  const stopRecording = async() => {
    if (stoppingRef.current) return; //ì´ë¯¸ ì •ì§€ ì²˜ë¦¬ì¤‘
    stoppingRef.current=true;

    console.log('ë…¹ìŒ ì¤‘ì§€');
    setIsRecording(false);

    // ì˜¤ë””ì˜¤ ì‹œìŠ¤í…œ ì •ë¦¬
    if (audioSystemRef.current) {
      const { stream, audioContext, processor } = audioSystemRef.current;
      await stopAudioRecognition(stream, audioContext, processor);
      audioSystemRef.current = null;
    }

    // WebSocketìœ¼ë¡œ ìŒì„± ë°œí™” ì¢…ë£Œ ì•Œë¦¼
    webSocketService.stopSpeaking();
    onListeningStop?.();
    stoppingRef.current=false;
  };

  const handleMicClick = () => {
    console.log("ğŸ¤ MicButton í´ë¦­ë¨");
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  // í˜„ì¬ ìƒíƒœì— ë”°ë¥¸ ë²„íŠ¼ ìŠ¤íƒ€ì¼ ê²°ì •
  const isActive = currentStep === 'listening' || isRecording;
    
  return (
    <button 
      onClick={handleMicClick} 
      className={`flex items-center px-[37px] py-[24px] text-[28px] font-bold w-[195px] h-[91px] rounded-[100px] border-[3px] border-white bg-yellow
        ${isActive 
          ? 'shadow-[0_0_80px_0_yellow]' //drop shadow ì ìš© 
          : ''
        }`}
      //disabled={isActive} // ì²˜ë¦¬ ì¤‘ì¼ ë•ŒëŠ” ë¹„í™œì„±í™”
      style={{overflow: 'visible'}} //shadow ì˜ë¦¼ ë°©ì§€
    >
      <img 
        src={isActive ? stopIcon : micIcon} 
        alt="Mic Icon" 
        className={isActive?"w-[15px] h-[15px] mr-[18px]":"w-[32px] h-[32px] mr-[4.5px]"}
      />
      {isActive ? 'ì¸ì‹ì¤‘' : 'ë§í•˜ê¸°'}
    </button>
  );
}