import { useState, useRef, useEffect } from "react";
import micIcon from "../../assets/images/mic_fill.svg";
import stopIcon from "../../assets/images/stop.svg";
import webSocketService from "../../service/websocketService";

// ìœ í‹¸: ì˜¤ë””ì˜¤ ìº¡ì²˜ & ë³€í™˜
function float32ToInt16(float32Array) {
  const int16Array = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    int16Array[i] = Math.max(-1, Math.min(1, float32Array[i])) * 0x7FFF;
  }
  return int16Array;
}

// ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘ (PCM16 ArrayBuffer ì½œë°±ìœ¼ë¡œ ì „ë‹¬)
async function startAudioRecognition(onAudioData) {
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: {
      sampleRate: 24000,
      channelCount: 1,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true
    }
  });

  const AC = window.AudioContext || window.webkitAudioContext;
  const audioContext = new AC({ sampleRate: 24000 });
  const source = audioContext.createMediaStreamSource(stream);
  const processor = audioContext.createScriptProcessor(4096, 1, 1);

  source.connect(processor);
  // processor.connect(audioContext.destination); // í•„ìš” ì—†ìœ¼ë©´ ì—°ê²° X

  processor.onaudioprocess = (e) => {
    const input = e.inputBuffer.getChannelData(0);
    const pcm = float32ToInt16(input);
    onAudioData?.(pcm.buffer); // ArrayBufferë¡œ ì „ë‹¬
  };

  return { stream, audioContext, processor };
}

// ì˜¤ë””ì˜¤ ìº¡ì²˜ ì •ì§€/ì •ë¦¬
async function stopAudioRecognition(stream, audioContext, processor) {
  try { if (processor) { processor.onaudioprocess = null; processor.disconnect(); } } catch {}
  try { if (stream) stream.getTracks().forEach(t => t.stop()); } catch {}
  try {
    if (audioContext && audioContext.state !== "closed") {
      await audioContext.close();
    }
  } catch {}
}

export default function MicButton({ onListeningStart, onListeningStop, onTranscriptUpdate, currentStep,onCallIntent }) {
  const [isRecording, setIsRecording] = useState(false);
  const audioSystemRef = useRef(null);
  //ì²«PCMì²­í¬ê°€ ì„œë²„ì— ë„ë‹¬í•˜ê¸° ì „ì— stopSpeaking()ì„ ì•ˆë³´ë‚´ë„ë¡
  const hasAudioRef=useRef(false); 
  const stoppingRef = useRef(false);
  const audioDataSentRef = useRef(false);

  // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì¶”ê°€
  const recognitionRef = useRef(null);
  const recogActiveRef = useRef(false);

  useEffect(() => {
    // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì´ˆê¸°í™”
    if ("webkitSpeechRecognition" in window || "SpeechRecognition" in window) {
      const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
      const rec = new SpeechRecognition();
      rec.continuous = false;
      rec.interimResults = false;   // ì¤‘ê°„ ê²°ê³¼ í•„ìš”ì‹œ true
      rec.lang = "ko-KR";

      rec.onresult = (event) => {
        const transcript = event.results[0][0]?.transcript || "";
        console.log("ğŸ¤ ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ê²°ê³¼:", transcript);
        onTranscriptUpdate?.(transcript);
        onListeningStop?.(transcript);
        if (/ì „í™”ë²ˆí˜¸|ì „í™”í•´|ì „í™” ê±¸ì–´|ì „í™”/i.test(transcript)) {
          try { window.dispatchEvent(new CustomEvent('sonju:call_intent', { detail: transcript })); } catch {}
          onCallIntent?.(transcript);
        }
      };

      rec.onerror = (event) => {
        console.error("ìŠ¤í”¼ì¹˜ ì¸ì‹ ì—ëŸ¬:", event.error);
      };

      rec.onstart = () => { recogActiveRef.current = true; };
      rec.onend   = () => { recogActiveRef.current = false; };

      recognitionRef.current = rec;
    }

    return () => {
      if (audioSystemRef.current) {
        const { stream, audioContext, processor } = audioSystemRef.current;
        stopAudioRecognition(stream, audioContext, processor);
      }
    };
  }, [onListeningStop, onTranscriptUpdate, onCallIntent]);

  const startRecording = async () => {
    try {
      if (stoppingRef.current) return;

      // WebSocket ì—°ê²° (í•„ìš” ì‹œ)
      if (!webSocketService.isConnected) {
        await webSocketService.connect(import.meta.env.VITE_WEBSOCKET_URL);
      }
      setIsRecording(true);
      onListeningStart?.();

      hasAudioRef.current = false;
      audioDataSentRef.current = false;

      // ì„œë²„ë¡œ â€œë…¹ìŒ ì‹œì‘â€ ì•Œë¦¼ (ì‹œì‘ ì‹œ commit ë³´ë‚´ì§€ ì•Šê¸°)
      webSocketService.startSpeaking();

      // ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘ í›„ ì²­í¬ë¥¼ ì„œë²„ë¡œ ì „ì†¡
      const audioSystem = await startAudioRecognition((arrayBuffer) => {
        const ok=webSocketService.sendAudioPCM16(arrayBuffer);
        if(ok) { 
          hasAudioRef.current=true; //ìµœì†Œ 1ì²­í¬ ë³´ëƒˆìŒ í‘œì‹œ
          audioDataSentRef.current=true;
        }
      });
      audioSystemRef.current = audioSystem;

      // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì‹œì‘ (ì¦‰ì‹œ í…ìŠ¤íŠ¸ ì–»ê¸°)
      try { recognitionRef.current?.start(); } catch {}

    } catch (e) {
      console.error("ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:", e);
      setIsRecording(false);
      alert("ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.");
    }
  };

  const stopRecording = async () => {
    if (stoppingRef.current) return;
    stoppingRef.current = true;

    setIsRecording(false);

    if (audioSystemRef.current) {
      const { stream, audioContext, processor } = audioSystemRef.current;
      await stopAudioRecognition(stream, audioContext, processor);
      audioSystemRef.current = null;
    }

    // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì •ì§€
    if (recogActiveRef.current) { try { recognitionRef.current?.stop(); } catch {} }

    // ì„œë²„ë¡œ ë…¹ìŒ ë ì•Œë¦¼ (commit -> end)
    //webSocketService.stopSpeaking(hasAudioRef.current);
    if (audioDataSentRef.current) {
      webSocketService.stopSpeaking(true);
    } else {
      webSocketService.stopSpeaking(false);
    }

    hasAudioRef.current=false;
    audioDataSentRef.current=false;
    stoppingRef.current = false;
  };

  const handleMicClick = () => {
    if (isRecording) stopRecording();
    else startRecording();
  };

  const isActive = isRecording || currentStep === "listening" || currentStep === "processing";

  return (
    <>
    <button 
      onClick={handleMicClick} 
      className={`flex items-center px-[37px] py-[24px] text-[28px] font-bold w-[195px] h-[91px] rounded-[100px] border-[3px] border-white bg-yellow
        ${isActive 
          ? 'shadow-[0_0_80px_0_yellow]' //drop shadow ì ìš©
          : ''
        }`} 
      //disabled={isActive} //ì²˜ë¦¬ ì¤‘ì¼ ë•ŒëŠ” ë¹„í™œì„±í™”
      style={{overflow: 'visible'}} //shadow ì˜ë¦¼ ë°©ì§€
      >
      <img 
        src={isActive ? stopIcon : micIcon} 
        alt="Mic" 
        className={isActive?"w-[15px] h-[15px] mr-[18px]":"w-[32px] h-[32px] mr-[4.5px]"}
      />
      {isActive ? "ì¸ì‹ì¤‘" : "ë§í•˜ê¸°"}
    </button>
    </>
  );
}
