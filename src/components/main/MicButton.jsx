import { useState, useRef, useEffect } from "react";
import micIcon from "../../assets/images/mic_fill.svg";
import stopIcon from "../../assets/images/stop.svg";
import webSocketService from "../../service/websocketService";

// ìœ í‹¸: ì˜¤ë””ì˜¤ ìº¡ì²˜ & ë³€í™˜
function float32ToInt16(float32Array) {
  const int16Array = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    int16Array[i] = Math.max(-1, Math.min(1, float32Array[i])) * 0x7FFF;
  }
  return int16Array;
}

// ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘ (PCM16 ArrayBuffer ì½œë°±ìœ¼ë¡œ ì „ë‹¬)
async function startAudioCapture(onAudioData) {
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: {
      sampleRate: 24000,
      channelCount: 1,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true
    }
  });

  const AC = window.AudioContext || window.webkitAudioContext;
  const audioContext = new AC({ sampleRate: 24000 });
  const source = audioContext.createMediaStreamSource(stream);
  const processor = audioContext.createScriptProcessor(4096, 1, 1);

  source.connect(processor);
  // processor.connect(audioContext.destination); // í•„ìš” ì—†ìœ¼ë©´ ì—°ê²° X

  processor.onaudioprocess = (e) => {
    const input = e.inputBuffer.getChannelData(0);
    const pcm = float32ToInt16(input);
    onAudioData?.(pcm.buffer); // ArrayBufferë¡œ ì „ë‹¬
  };

  return { stream, audioContext, processor };
}

// ì˜¤ë””ì˜¤ ìº¡ì²˜ ì •ì§€
async function stopAudioCapture(stream, audioContext, processor) {
  try {
    if (processor) {
      processor.onaudioprocess = null;
      processor.disconnect();
    }
  } catch (e) {
    console.warn('Processor disconnect error:', e);
  }

  try {
    if (stream) {
      stream.getTracks().forEach(track => track.stop());
    }
  } catch (e) {
    console.warn('Stream stop error:', e);
  }

  try {
    if (audioContext && audioContext.state !== "closed") {
      await audioContext.close();
    }
  } catch (e) {
    console.warn('Audio context close error:', e);
  }
}

export default function MicButton({ 
  onListeningStart, 
  onListeningStop, 
  onTranscriptUpdate, 
  currentStep 
}) {
  const [isRecording, setIsRecording] = useState(false);
  const audioSystemRef = useRef(null);
  //ì²«PCMì²­í¬ê°€ ì„œë²„ì— ë„ë‹¬í•˜ê¸° ì „ì— stopSpeaking()ì„ ì•ˆë³´ë‚´ë„ë¡
  const hasAudioRef=useRef(false); 
  const stoppingRef = useRef(false);
  const audioDataCountRef = useRef(0);

  // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì¶”ê°€
  const recognitionRef = useRef(null);
  const recogActiveRef = useRef(false);

  useEffect(() => {
    // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì´ˆê¸°í™”
    if ("webkitSpeechRecognition" in window || "SpeechRecognition" in window) {
      const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
      const rec = new SpeechRecognition();
      rec.continuous = false;
      rec.interimResults = false;   // ì¤‘ê°„ ê²°ê³¼ í•„ìš”ì‹œ true
      rec.lang = "ko-KR";

      rec.onresult = (event) => {
        const transcript = event.results[0][0]?.transcript || "";
        console.log("ğŸ¤ ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ê²°ê³¼:", transcript);
        onTranscriptUpdate?.(transcript);
        onListeningStop?.(transcript);
      };

      rec.onerror = (event) => {
        console.error("ìŠ¤í”¼ì¹˜ ì¸ì‹ ì—ëŸ¬:", event.error);
      };

      rec.onstart = () => { 
        recogActiveRef.current = true; 
        console.log("ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì‹œì‘");
      };
      
      rec.onend = () => { 
        recogActiveRef.current = false; 
        console.log("ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì¢…ë£Œ");
      };

      recognitionRef.current = rec;
    }

    return () => {
      if (audioSystemRef.current) {
        const { stream, audioContext, processor } = audioSystemRef.current;
        stopAudioCapture(stream, audioContext, processor);
      }
    };
  }, [onListeningStop, onTranscriptUpdate]);

  const startRecording = async () => {
    try {
      if (stoppingRef.current) return;

      console.log("ğŸ¤ ìŒì„± ë…¹ìŒ ì‹œì‘");

      // WebSocket ì—°ê²° í™•ì¸
      if (!webSocketService.isConnected) {
        await webSocketService.connect(import.meta.env.VITE_WEBSOCKET_URL);
      }

      setIsRecording(true);
      onListeningStart?.();
      hasAudioRef.current = false;
      audioDataCountRef.current = 0;

      // 1ë‹¨ê³„: ì„œë²„ì— ìŒì„± ë°œí™” ì‹œì‘ ì•Œë¦¼ (commit)
      //const commitSent = webSocketService.startSpeaking();
      //console.log("ğŸ“¤ input_audio_buffer.commit ì „ì†¡:", commitSent);
      webSocketService.startSpeaking();

      // 2ë‹¨ê³„: ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘ ë° ì‹¤ì‹œê°„ ì „ì†¡
      const audioSystem = await startAudioCapture((arrayBuffer) => {
        const sent = webSocketService.sendAudioPCM16(arrayBuffer);
        if (sent) {
          hasAudioRef.current = true;
          audioDataCountRef.current++;
        }
      });
      audioSystemRef.current = audioSystem;

      // 3ë‹¨ê³„: ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì‹œì‘ (ë°±ì—…)
      try {
        if (recognitionRef.current) {
          recognitionRef.current.start();
        }
      } catch (e) {
        console.warn("ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:", e);
      }

    } catch (e) {
      console.error("ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:", e);
      setIsRecording(false);
      alert("ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.");
    }
  };

  const stopRecording = async () => {
    if (stoppingRef.current) return;
    stoppingRef.current = true;

    console.log("ğŸ›‘ ìŒì„± ë…¹ìŒ ì¤‘ì§€");

    setIsRecording(false);

    // ì˜¤ë””ì˜¤ ìº¡ì²˜ ì¤‘ì§€
    if (audioSystemRef.current) {
      const { stream, audioContext, processor } = audioSystemRef.current;
      await stopAudioCapture(stream, audioContext, processor);
      audioSystemRef.current = null;
    }

    // ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì¤‘ì§€
    if (recogActiveRef.current) {
      try {
        recognitionRef.current?.stop();
      } catch (e) {
        console.warn("ë¸Œë¼ìš°ì € ìŠ¤í”¼ì¹˜ ì¸ì‹ ì¤‘ì§€ ì‹¤íŒ¨:", e);
      }
    }

    // ì„œë²„ì— ìŒì„± ë°œí™” ì¢…ë£Œ ì•Œë¦¼
    const endSent = webSocketService.stopSpeaking(hasAudioRef.current);
    console.log("ğŸ“¤ input_audio_buffer.end ì „ì†¡:", endSent);
    
    hasAudioRef.current = false;
    audioDataCountRef.current = 0;
    stoppingRef.current = false;

    // ìƒìœ„ ì»´í¬ë„ŒíŠ¸ì— ì¤‘ì§€ ì•Œë¦¼
    onListeningStop?.();
  };

  const handleMicClick = () => {
    if (isRecording) stopRecording();
    else startRecording();
  };

  const isActive = isRecording || currentStep === "listening" || currentStep === "processing";

  return (
    <>
    <button 
      onClick={handleMicClick} 
      className={`flex items-center px-[37px] py-[24px] text-[28px] font-bold w-[195px] h-[91px] rounded-[100px] border-[3px] border-white bg-yellow
        ${isActive 
          ? 'shadow-[0_0_80px_0_yellow]' //drop shadow ì ìš©
          : ''
        }`} 
      //disabled={isActive} //ì²˜ë¦¬ ì¤‘ì¼ ë•ŒëŠ” ë¹„í™œì„±í™”
      style={{overflow: 'visible'}} //shadow ì˜ë¦¼ ë°©ì§€
    >
      <img 
        src={isActive ? stopIcon : micIcon} 
        alt="Mic" 
        className={isActive?"w-[15px] h-[15px] mr-[18px]":"w-[32px] h-[32px] mr-[4.5px]"}
      />
      {isActive ? "ì¸ì‹ì¤‘" : "ë§í•˜ê¸°"}
    </button>
    </>
  );
}
